{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76764285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import vector; vector.register_awkward()\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "from coffea import processor\n",
    "from coffea.processor import servicex\n",
    "from coffea.nanoevents import transforms\n",
    "from coffea.nanoevents.methods import base, vector\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from func_adl import ObjectStream\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "#import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6c4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 5\n",
    "\n",
    "# pipeline to use:\n",
    "# - \"coffea\" for pure coffea setup\n",
    "# - \"servicex_processor\" for coffea with ServiceX processor\n",
    "# - \"servicex_databinder\" for downloading query output and subsequent standalone coffea\n",
    "PIPELINE = \"coffea\"\n",
    "\n",
    "# enable Dask (may not work yet in combination with ServiceX outside of coffea-casa)\n",
    "USE_DASK = False\n",
    "\n",
    "# ServiceX behavior: ignore cache with repeated queries\n",
    "SERVICEX_IGNORE_CACHE = False\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "#AF = \"coffea_casa\"\n",
    "AF = \"local\"\n",
    "\n",
    "### BENCHMARKING-SPECIFIC SETTINGS\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# metadata to propagate through to metrics\n",
    "AF_NAME = \"coffea_casa\"  # \"ssl-dev\" allows for the switch to local data on /data\n",
    "SYSTEMATICS = \"all\"  # currently has no effect\n",
    "CORES_PER_WORKER = 2  # does not do anything, only used for metric gathering (set to 2 for distributed coffea-casa)\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# only I/O, all other processing disabled\n",
    "DISABLE_PROCESSING = False\n",
    "\n",
    "# read additional branches (only with DISABLE_PROCESSING = True)\n",
    "# acceptable values are 2.7, 4, 15, 25, 50 (corresponding to % of file read), 2.7% corresponds to the standard branches used in the notebook\n",
    "IO_FILE_PERCENT = 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a76cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_fileset(n_files_max_per_sample, \n",
    "                      use_xcache=False, \n",
    "                      af_name=\"\", \n",
    "                      onlyNominal=False,\n",
    "                      ntuples_json=\"ntuples_nanoaod.json\"):\n",
    "    # using https://atlas-groupdata.web.cern.ch/atlas-groupdata/dev/AnalysisTop/TopDataPreparation/XSection-MC15-13TeV.data\n",
    "    # for reference\n",
    "    # x-secs are in pb\n",
    "    xsec_info = {\n",
    "        \"ttbar\": 396.87 + 332.97, # nonallhad + allhad, keep same x-sec for all\n",
    "        \"single_top_s_chan\": 2.0268 + 1.2676,\n",
    "        \"single_top_t_chan\": (36.993 + 22.175)/0.252,  # scale from lepton filter to inclusive\n",
    "        \"single_top_tW\": 37.936 + 37.906,\n",
    "        \"wjets\": 61457 * 0.252,  # e/mu+nu final states\n",
    "        \"data\": None\n",
    "    }\n",
    "\n",
    "    # list of files\n",
    "    with open(ntuples_json) as f:\n",
    "        file_info = json.load(f)\n",
    "\n",
    "    # process into \"fileset\" summarizing all info\n",
    "    fileset = {}\n",
    "    for process in file_info.keys():\n",
    "        if process == \"data\":\n",
    "            continue  # skip data\n",
    "\n",
    "        for variation in file_info[process].keys():\n",
    "            if onlyNominal: \n",
    "                if variation.startswith(\"nominal\"): continue\n",
    "            print(variation)\n",
    "            file_list = file_info[process][variation][\"files\"]\n",
    "            if n_files_max_per_sample != -1:\n",
    "                file_list = file_list[:n_files_max_per_sample]  # use partial set of samples\n",
    "\n",
    "            file_paths = [f[\"path\"] for f in file_list]\n",
    "            nevts_total = sum([f[\"nevts\"] for f in file_list])\n",
    "            metadata = {\"process\": process, \"variation\": variation, \"nevts\": nevts_total, \"xsec\": xsec_info[process]}\n",
    "            fileset.update({f\"{process}__{variation}\": {\"files\": file_paths, \"metadata\": metadata}})\n",
    "\n",
    "    return fileset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6873da60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal\n",
      "scaledown\n",
      "scaleup\n",
      "ME_var\n",
      "PS_var\n",
      "nominal\n",
      "nominal\n",
      "nominal\n",
      "scaledown\n",
      "scaleup\n",
      "DS\n",
      "nominal\n",
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'single_top_tW__scaledown', 'single_top_tW__scaleup', 'single_top_tW__DS', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 6389801, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False, \n",
    "                            onlyNominal=False,\n",
    "                                  af_name=AF_NAME, \n",
    "                                  ntuples_json='/eos/user/a/algomez/SWAN_projects/analysis-grand-challenge-emk/analyses/cms-open-data-ttbar/ntuples_nanoaod.json')  # local files on /data for ssl-dev\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TtbarAnalysis(processor.processorABC):\n",
    "    def __init__(self, disable_processing, io_file_percent):\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "        self.disable_processing = disable_processing\n",
    "        self.io_file_percent = io_file_percent\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        # standard AGC branches cover 2.7% of the data\n",
    "            branches_to_read = []\n",
    "            if self.io_file_percent >= 2.7:\n",
    "                branches_to_read.extend([\"Jet_pt\", \"Jet_eta\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_mass\", \"Muon_pt\", \"Electron_pt\"])\n",
    "            \n",
    "            if self.io_file_percent >= 4:\n",
    "                branches_to_read.extend([\"Electron_phi\", \"Electron_eta\",\"Electron_mass\",\"Muon_phi\",\"Muon_eta\",\"Muon_mass\",\n",
    "                                         \"Photon_pt\",\"Photon_eta\",\"Photon_mass\",\"Jet_jetId\"])\n",
    "            \n",
    "            if self.io_file_percent>=15:\n",
    "                branches_to_read.extend([\"Jet_nConstituents\",\"Jet_electronIdx1\",\"Jet_electronIdx2\",\"Jet_muonIdx1\",\"Jet_muonIdx2\",\n",
    "                                         \"Jet_chHEF\",\"Jet_area\",\"Jet_puId\",\"Jet_qgl\",\"Jet_btagDeepB\",\"Jet_btagDeepCvB\",\n",
    "                                         \"Jet_btagDeepCvL\",\"Jet_btagDeepFlavB\",\"Jet_btagDeepFlavCvB\",\"Jet_btagDeepFlavCvL\",\n",
    "                                         \"Jet_btagDeepFlavQG\",\"Jet_chEmEF\",\"Jet_chFPV0EF\",\"Jet_muEF\",\"Jet_muonSubtrFactor\",\n",
    "                                         \"Jet_neEmEF\",\"Jet_neHEF\",\"Jet_puIdDisc\"])\n",
    "            \n",
    "            if self.io_file_percent>=25:\n",
    "                branches_to_read.extend([\"Jet_rawFactor\",\"Jet_bRegCorr\",\"Jet_bRegRes\",\"Jet_cRegCorr\",\"Jet_cRegRes\",\"Jet_nElectrons\",\n",
    "                                         \"Jet_nMuons\",\"GenJet_pt\",\"GenJet_eta\",\"GenJet_phi\",\"GenJet_mass\",\"Tau_pt\",\"Tau_eta\",\"Tau_mass\",\n",
    "                                         \"Tau_phi\",\"Muon_dxy\",\"Muon_dxyErr\",\"Muon_dxybs\",\"Muon_dz\",\"Muon_dzErr\",\"Electron_dxy\",\n",
    "                                         \"Electron_dxyErr\",\"Electron_dz\",\"Electron_dzErr\",\"Electron_eInvMinusPInv\",\"Electron_energyErr\",\n",
    "                                         \"Electron_hoe\",\"Electron_ip3d\",\"Electron_jetPtRelv2\",\"Electron_jetRelIso\",\n",
    "                                         \"Electron_miniPFRelIso_all\",\"Electron_miniPFRelIso_chg\",\"Electron_mvaFall17V2Iso\",\n",
    "                                         \"Electron_mvaFall17V2noIso\",\"Electron_pfRelIso03_all\",\"Electron_pfRelIso03_chg\",\"Electron_r9\",\n",
    "                                         \"Electron_scEtOverPt\",\"Electron_sieie\",\"Electron_sip3d\",\"Electron_mvaTTH\",\"Electron_charge\",\n",
    "                                         \"Electron_cutBased\",\"Electron_jetIdx\",\"Electron_pdgId\",\"Electron_photonIdx\",\"Electron_tightCharge\"])\n",
    "            \n",
    "            if self.io_file_percent==50:\n",
    "                branches_to_read.extend([\"GenPart_pt\",\"GenPart_eta\",\"GenPart_phi\",\"GenPart_mass\",\"GenPart_genPartIdxMother\",\n",
    "                                         \"GenPart_pdgId\",\"GenPart_status\",\"GenPart_statusFlags\"])\n",
    "                \n",
    "            if self.io_file_percent not in [2.7, 4, 15, 25, 50]:\n",
    "                raise NotImplementedError(\"supported values for I/O percentage are 4, 15, 25, 50\")\n",
    "            \n",
    "            for branch in branches_to_read:\n",
    "                if \"_\" in branch:\n",
    "                    split = branch.split(\"_\")\n",
    "                    object_type = split[0]\n",
    "                    property_name = '_'.join(split[1:])\n",
    "                    ak.materialized(events[object_type][property_name])\n",
    "                else:\n",
    "                    ak.materialized(events[branch])\n",
    "            return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if self.disable_processing:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        histogram = self.hist.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        #### systematics\n",
    "        # example of a simple flat weight variation, using the coffea nanoevents systematics feature\n",
    "        if process == \"wjets\":\n",
    "            events.add_systematic(\"scale_var\", \"UpDownSystematic\", \"weight\", flat_variation)\n",
    "\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "        pt_variations = [\"pt_nominal\", \"pt_scale_up\", \"pt_res_up\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # pT > 25 GeV for leptons & jets\n",
    "            selected_electrons = events.Electron[events.Electron.pt > 25]\n",
    "            selected_muons = events.Muon[events.Muon.pt > 25]\n",
    "            jet_filter = events.Jet.pt * events[pt_var] > 25  # pT > 25 GeV for jets (scaled by systematic variations)\n",
    "            selected_jets = events.Jet[jet_filter]\n",
    "\n",
    "            # single lepton requirement\n",
    "            event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # at least four jets\n",
    "            pt_var_modifier = events[pt_var] if \"res\" not in pt_var else events[pt_var][jet_filter]\n",
    "            event_filters = event_filters & (ak.count(selected_jets.pt * pt_var_modifier, axis=1) >= 4)\n",
    "            # at least one b-tagged jet (\"tag\" means score above threshold)\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n",
    "                if region == \"4j1b\":\n",
    "                    region_filter = ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) == 1\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "                    # use HT (scalar sum of jet pT) as observable\n",
    "                    pt_var_modifier = (\n",
    "                        events[event_filters][region_filter][pt_var]\n",
    "                        if \"res\" not in pt_var\n",
    "                        else events[pt_var][jet_filter][event_filters][region_filter]\n",
    "                    )\n",
    "                    observable = ak.sum(selected_jets_region.pt * pt_var_modifier, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "                    region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "                    trijet = ak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                ### histogram filling\n",
    "                if pt_var == \"pt_nominal\":\n",
    "                    # nominal pT, but including 2-point systematics\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=variation, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "                    if variation == \"nominal\":\n",
    "                        # also fill weight-based variations for all nominal samples\n",
    "                        for weight_name in events.systematics.fields:\n",
    "                            for direction in [\"up\", \"down\"]:\n",
    "                                # extract the weight variations and apply all event & region filters\n",
    "                                weight_variation = events.systematics[weight_name][direction][\n",
    "                                    f\"weight_{weight_name}\"][event_filters][region_filter]\n",
    "                                # fill histograms\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                        # calculate additional systematics: b-tagging variations\n",
    "                        for i_var, weight_name in enumerate([f\"btag_var_{i}\" for i in range(4)]):\n",
    "                            for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                                # create systematic variations that depend on object properties (here: jet pT)\n",
    "                                if len(observable):\n",
    "                                    weight_variation = btag_weight_variation(i_var, selected_jets_region.pt)[:, i_dir]\n",
    "                                else:\n",
    "                                    weight_variation = 1 # no events selected\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                elif variation == \"nominal\":\n",
    "                    # pT variations for nominal samples\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=pt_var, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
